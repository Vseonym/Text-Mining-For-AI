{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f37005",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Comparison: spaCy vs Stanza\n",
    "\n",
    "This notebook demonstrates how to extract named entities from sentences using two different NLP libraries: **spaCy** and **Stanza**. We use a token-level test set and compare the results of both systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6ff83",
   "metadata": {},
   "source": [
    "## 1. Load the NER Test Data\n",
    "\n",
    "We load the token-level NER test set, which contains columns for sentence ID, token ID, token, and BIO NER tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Use pandas to read the dataset\n",
    "ner_test = pd.read_csv(r'NER-test.tsv', sep=\"\\t\") # Read the dataset\n",
    "ner_test.head() # Look at the first 5 rows to ensure the data is read correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418eff8",
   "metadata": {},
   "source": [
    "## 2. Reconstruct Sentences\n",
    "\n",
    "Since the data is tokenized, we group tokens by `sentence_id` to reconstruct the full sentences for NER processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ner_test.groupby('sentence_id')['token'].apply(lambda tokens: ' '.join(tokens)).reset_index() # Group the data by sentence ID and join the tokens into a single sentence\n",
    "sentences.columns = ['sentence_id', 'sentence'] # Rename the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a01fb7",
   "metadata": {},
   "source": [
    "## 3. Import and Initialize NLP Libraries\n",
    "\n",
    "We import and initialize the spaCy and Stanza pipelines for English. These will be used to extract named entities from each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\") # Load the English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download(\"en\") # Download the English language model\n",
    "nlp_stanza = stanza.Pipeline(\"en\") # Load the English language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890a51f",
   "metadata": {},
   "source": [
    "## 4. Define Entity Extraction Functions\n",
    "\n",
    "We define helper functions to extract entities from a sentence using each library. The functions return a list of (entity text, entity label) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_spacy(text): # Function to extract entities using spaCy \n",
    "    doc = nlp_spacy(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1eac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_stanza(text): # Function to extract entities using stanza\n",
    "    doc = nlp_stanza(text)\n",
    "    return [(ent.text, ent.type) for sent in doc.sentences for ent in sent.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ac953",
   "metadata": {},
   "source": [
    "## 5. Apply NER Systems\n",
    "\n",
    "We apply both spaCy and Stanza NER to each sentence and store the results in new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['spacy_entities'] = sentences['sentence'].apply(extract_entities_spacy) # Apply the spaCy function to each sentence\n",
    "sentences['stanza_entities'] = sentences['sentence'].apply(extract_entities_stanza) # Apply the stanza function to each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162d1c3",
   "metadata": {},
   "source": [
    "## 6. Display and Compare Results\n",
    "\n",
    "We display the sentences alongside the entities extracted by each system. This allows for direct comparison and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in sentences.iterrows():\n",
    "    print(f\"Sentence: {row['sentence']}\\n\") # Print the sentence\n",
    "    print(\"spaCy entities:\")\n",
    "    for ent in row['spacy_entities']: # Iterate over the spaCy entities and print them\n",
    "        print(f\"  {ent[0]} ({ent[1]})\")\n",
    "    print(\"Stanza entities:\")\n",
    "    for ent in row['stanza_entities']: # Iterate over the stanza entities and print them\n",
    "        print(f\"  {ent[0]} ({ent[1]})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021db98d",
   "metadata": {},
   "source": [
    "## 7. Direct Comparison of spaCy and Stanza Results\n",
    "\n",
    "The following sections present a direct comparison between the named entity recognition results produced by spaCy and Stanza. We will analyze agreements, disagreements, and unique findings from each system side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2bf05",
   "metadata": {},
   "source": [
    "A table containing all the results of the models side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0440b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the comparison DataFrame to see spaCy and Stanza side by side\n",
    "pivot_df = comparison_df.pivot_table(\n",
    "    index=['sentence_id', 'sentence', 'entity'],\n",
    "    columns='system',\n",
    "    values='label',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2da210",
   "metadata": {},
   "source": [
    "The number of entities found by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912402d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities found by each model\n",
    "n_spacy = comparison_df[comparison_df['system'] == 'spaCy'].shape[0]\n",
    "n_stanza = comparison_df[comparison_df['system'] == 'Stanza'].shape[0]\n",
    "\n",
    "print(f\"Entities found by spaCy: {n_spacy}\")\n",
    "print(f\"Entities found by Stanza: {n_stanza}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ee502",
   "metadata": {},
   "source": [
    "The number of entities with same or different label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities found by both systems (side-by-side comparison)\n",
    "filtered = pivot_df.dropna(subset=['spaCy', 'Stanza'], how='all')\n",
    "agreement = filtered[filtered['spaCy'] == filtered['Stanza']]\n",
    "disagreement = filtered[filtered['spaCy'] != filtered['Stanza']]\n",
    "\n",
    "print(f\"Entities with SAME label: {len(agreement)}\")\n",
    "print(f\"Entities with DIFFERENT label: {len(disagreement)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735dfda1",
   "metadata": {},
   "source": [
    "Uniqueness of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3880800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities found only by spaCy\n",
    "only_spacy = pivot_df[(pivot_df['spaCy'].notna()) & (pivot_df['Stanza'].isna())]\n",
    "# Entities found only by Stanza\n",
    "only_stanza = pivot_df[(pivot_df['Stanza'].notna()) & (pivot_df['spaCy'].isna())]\n",
    "\n",
    "print(f\"Entities found ONLY by spaCy: {len(only_spacy)}\")\n",
    "print(f\"Entities found ONLY by Stanza: {len(only_stanza)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6676a4a",
   "metadata": {},
   "source": [
    "Previously mentioned comparisons in table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Table of agreement:\")\n",
    "display(agreement)\n",
    "\n",
    "print(\"Table of disagreement:\")\n",
    "display(disagreement)\n",
    "\n",
    "print(\"Table of entities only found by spaCy:\")\n",
    "display(only_spacy)\n",
    "\n",
    "print(\"Table of entities only found by Stanza:\")\n",
    "display(only_stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdce72",
   "metadata": {},
   "source": [
    "Distribution of entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of entity types per model\n",
    "spacy_types = comparison_df[comparison_df['system'] == 'spaCy']['label'].value_counts()\n",
    "stanza_types = comparison_df[comparison_df['system'] == 'Stanza']['label'].value_counts()\n",
    "\n",
    "print(\"spaCy entity type distribution:\")\n",
    "print(spacy_types)\n",
    "print(\"\\nStanza entity type distribution:\")\n",
    "print(stanza_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775a413",
   "metadata": {},
   "source": [
    "## 8. Performance analysis of both models\n",
    "\n",
    "Since the NER-test.tsv file includes BIO_NER_tags (gold data), we can use these tags to analyse the performance of the two models. This gives an idea on the correctness of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b841112",
   "metadata": {},
   "source": [
    "Analyse performance on true positives, false positives and false negatives. Performance measured by precision, recall and f1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_spans(tokens, tags): # Function to convert BIO tags to entity spans\n",
    "    \"\"\"Convert BIO tags to entity spans: (start, end, label, text)\"\"\"\n",
    "    spans = []\n",
    "    start = None\n",
    "    label = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.startswith('B-'):\n",
    "            if start is not None:\n",
    "                spans.append((start, i, label, ' '.join(tokens[start:i])))\n",
    "            start = i\n",
    "            label = tag[2:]\n",
    "        elif tag.startswith('I-'):\n",
    "            continue\n",
    "        else:  # tag == 'O'\n",
    "            if start is not None:\n",
    "                spans.append((start, i, label, ' '.join(tokens[start:i])))\n",
    "                start = None\n",
    "                label = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(tags), label, ' '.join(tokens[start:len(tags)])))\n",
    "    return spans\n",
    "\n",
    "# Build gold spans for each sentence\n",
    "gold_spans = {}\n",
    "for sid, group in ner_test.groupby('sentence_id'):\n",
    "    tokens = group['token'].tolist()\n",
    "    tags = group['BIO_NER_tag'].tolist()\n",
    "    spans = bio_to_spans(tokens, tags)\n",
    "    gold_spans[sid] = set((span[3], span[2]) for span in spans)  # (text, label)\n",
    "\n",
    "# Build predicted spans for each model\n",
    "def get_predicted_spans(row, col):\n",
    "    return set((ent[0], ent[1]) for ent in row[col])\n",
    "\n",
    "sentences['gold_spans'] = sentences['sentence_id'].map(gold_spans)\n",
    "sentences['spacy_spans'] = sentences['spacy_entities'].apply(lambda ents: set(ents))\n",
    "sentences['stanza_spans'] = sentences['stanza_entities'].apply(lambda ents: set(ents))\n",
    "\n",
    "# Evaluate for each model\n",
    "def evaluate(pred_col):\n",
    "    tp = 0  # true positives\n",
    "    fp = 0  # false positives\n",
    "    fn = 0  # false negatives\n",
    "    for _, row in sentences.iterrows():\n",
    "        gold = row['gold_spans']\n",
    "        pred = set((text, label) for text, label in row[pred_col])\n",
    "        tp += len(gold & pred)\n",
    "        fp += len(pred - gold)\n",
    "        fn += len(gold - pred)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "spacy_tp, spacy_fp, spacy_fn, spacy_prec, spacy_rec, spacy_f1 = evaluate('spacy_entities')\n",
    "stanza_tp, stanza_fp, stanza_fn, stanza_prec, stanza_rec, stanza_f1 = evaluate('stanza_entities')\n",
    "\n",
    "print(\"spaCy:\")\n",
    "print(f\"  TP: {spacy_tp}, FP: {spacy_fp}, FN: {spacy_fn}\")\n",
    "print(f\"  Precision: {spacy_prec:.2f}, Recall: {spacy_rec:.2f}, F1: {spacy_f1:.2f}\")\n",
    "\n",
    "print(\"\\nStanza:\")\n",
    "print(f\"  TP: {stanza_tp}, FP: {stanza_fp}, FN: {stanza_fn}\")\n",
    "print(f\"  Precision: {stanza_prec:.2f}, Recall: {stanza_rec:.2f}, F1: {stanza_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['TP', 'FP', 'FN']\n",
    "spacy_scores = [spacy_tp, spacy_fp, spacy_fn]\n",
    "stanza_scores = [stanza_tp, stanza_fp, stanza_fn]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(x, spacy_scores, width=0.35, label='spaCy', align='center')\n",
    "plt.bar([i + 0.35 for i in x], stanza_scores, width=0.35, label='Stanza', align='center')\n",
    "plt.xticks([i + 0.175 for i in x], metrics)\n",
    "plt.ylabel('Count')\n",
    "plt.title('NER Model Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc49bda",
   "metadata": {},
   "source": [
    "NER Model performance visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45be6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Precision', 'Recall', 'F1']\n",
    "spacy_scores = [spacy_prec, spacy_rec, spacy_f1]\n",
    "stanza_scores = [stanza_prec, stanza_rec, stanza_f1]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(x, spacy_scores, width=0.35, label='spaCy', align='center')\n",
    "plt.bar([i + 0.35 for i in x], stanza_scores, width=0.35, label='Stanza', align='center')\n",
    "plt.xticks([i + 0.175 for i in x], metrics)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.title('NER Model Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
